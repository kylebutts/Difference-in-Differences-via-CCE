\documentclass[12pt,fleqn]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{a4wide}
\usepackage{lscape}
\usepackage{mathpazo}
\usepackage{rotating}
\usepackage{subcaption}
\usepackage{float}
\usepackage{epstopdf}
\usepackage{rotating}
\usepackage{adjustbox}
\usepackage{setspace}
\usepackage[margin=0.85in,paper=letterpaper]{geometry}
\usepackage[flushleft]{threeparttable}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{dsfont}
\usepackage{bm}
\usepackage{tikz}
\def\*#1{\mathbf{#1}}
\def\+#1{\boldsymbol{#1}}

\makeatletter
\makeatother

\usepackage[english]{babel}
\usepackage[round]{natbib}
\bibliographystyle{ecta}
%\usepackage{hyperref}
%\hypersetup{
%    colorlinks=true, linkcolor=blue, filecolor=magenta,
%    urlcolor=cyan, citecolor=blue, pdftitle={TO BE ADDED}
%}



\begin{document}

\title{\textsc{Online Appendix to ``Simple Difference-in-Differences Estimation in Fixed-$T$ Panels''}}
\author{\and Nicholas Brown\\
{\small Queen's University} \and Kyle Butts\\
{\small University of Colorado Boulder} \and Joakim Westerlund\thanks{Corresponding author: Department of Economics, Lund University, Box 7082, 220 07 Lund, Sweden. Telephone: +46 46 222 8997. Fax: +46 46 222 4613. E-mail address: \texttt{joakim.westerlund@nek.lu.se}.}\\
{\small Lund University}\\
{\small and}\\
{\small Deakin University}
\date{May 24, 2023}}

\maketitle
%\doublespacing

\begin{abstract}
\setlength{\baselineskip}{0.83cm}
This appendix provides (i) the proof of Theorem 1 reported in the main paper, and (ii) discussion of some of the assumptions.
\end{abstract}

\setlength{\baselineskip}{0.83cm}

\section{Proof of Theorem 1}
\renewcommand{\theequation}{A.\arabic{equation}}

We start with part (a) of the theorem. We begin by considering the step-1 estimator of $\*f_t$. In so doing, it is useful to denote by $\overline{\*a}_t = (|\mathcal{I}_{\infty}|)^{-1}\sum_{i \in \mathcal{I}_{\infty}} \*a_{i,t}$ the cross-sectional average of any vector $\*a_{i,t}$ for the group of untreated units ($g= \infty$). In this notation, $\widehat{\*{f}}_t = \overline {\*z}_t$. Making use of this and the expression given for $\*z_{i,t}$ in the main paper,
\begin{equation}\label{fhat1}
\widehat{\*{f}}_t = \overline {\*{z}}_t = \overline{\+\Lambda}'\*{f}_t + \overline {\*e}_{t}
\end{equation}
for the pretreatment sample $t \leq g_{\min}$. Here, $\overline{\+\Lambda}$ and $\overline {\*e}_{t}$ are the cross-sectional averages of $\+\Lambda_i = [\+\alpha_i +\+\lambda_i\+\beta_i, \+\lambda_i]$ and $\*e_{i,t} = [\varepsilon_{i,t} + \+\beta_i'\*v_{i,t}, \*v_{i,t}']'$, respectively. If $m+1=r$, then the $r\times (m+1)$ matrix $\overline{\+\Lambda}$ is square and invertible, which means that \eqref{fhat1} can be rewritten as
\begin{equation}\label{redefined}
    \overline{\+\Lambda}^{-1\prime}\widehat{\*{f}}_t=\*{f}_t+\overline{\+\Lambda}^{-1\prime}\overline{\*e}_t .
\end{equation}
Hence, because $\|\overline{\*e}_t\|=O_p(N^{-1/2})$ under Assumption 4, we have
\begin{equation}\label{redefined2}
\overline{\+\Lambda}^{-1\prime}\widehat{\*{f}}_t = \*{f}_t + O_p(N^{-1/2})
\end{equation}
and hence $\overline{\+\Lambda}^{-1\prime} \widehat{\*{f}}_t$ is consistent for $\*{f}_t$. In practice, we never observe $\overline{\+\Lambda}$. However, since $\+{\alpha}_i'\*{f}_t = \+{\alpha}_i' \overline{\+\Lambda}^{-1\prime}\widehat{\*{f}}_t + O_p(N^{-1/2})$, it is enough if we know $\widehat{\*{f}}_t$, because $\overline{\+\Lambda}^{-1}$ is subsumed in the estimation of the coefficient of $\widehat{\*{f}}_t$, which is $\*a_i$ in our notation.

The above analysis is not possible when $m +1> r$ since $\overline{\+{\Lambda}}$ is no longer invertible. However, we still need something similar to (\ref{redefined}), because it determines the object that is being estimated. The way we approach this issue is the same as in \citet{westerlund2019cce}, and others. In particular, we begin by partitioning $\+{\Lambda}_i$ as $\overline{\+{\Lambda}}=[\overline{\+{\Lambda}}_r, \overline{\+{\Lambda}}_{-r}]$, where $\overline{\+{\Lambda}}_{-r}$ is $r\times (m+1-r)$ and $\overline{\+{\Lambda}}_r$ is $r \times r$ and full rank. Note that this partition is without loss of generality under Assumption 6. We then introduce the following $(m+1)\times (m+1)$ rotation matrix, which is chosen such that $\overline{\+{\Lambda}}\overline{\*{H}}= [\*{I}_r, \*{0}_{r\times (m+1-r)}]$ and that is going to play the same role as $\overline{\+{\Lambda}}^{-1}$ under $m+1=r$:
\begin{equation}
    \overline{\*{H}} =\left[\begin{array}{cc}\overline{\+{\Lambda}}_r^{-1} & -\overline{\+{\Lambda}}_r^{-1}\overline{\+{\Lambda}}_{-r}\\
    \*{0}_{(m+1-r)\times r} & \*{I}_{m+1-r}\end{array}\right]=[\overline{\*{H}}_r,\, \overline{\*{H}}_{-r}],
\end{equation}
where $\overline{\*{H}}_r=[\overline{\+{\Lambda}}_r^{-1\prime}, \*{0}_{r\times (m+1-r)} ]'$ is $(m+1)\times r$, while $\overline{\*{H}}_{-r}=[ -\overline{\+{\Lambda}}_{-r}'\overline{\+{\Lambda}}_r^{-1\prime}, \*{I}_{m+1-r}]'$ is $(m+1)\times (m+1-r)$. If $m+1=r$, we define $\overline{\*{H}}=\overline{\*{H}}_r=\overline{\+{\Lambda}}_r^{-1}=\overline{\+{\Lambda}}^{-1}$. We further introduce the $(m+1)\times (m+1)$ matrix $\*{D}_N= \mathrm{diag}(\*{I}_r , \sqrt{N}\*{I}_{m+1-r})$ with $\*{D}_N=\*{I}_{m+1}$ if $m+1=r$. By pre-multiplying $\widehat{\*{f}}_t$ by $\*{D}_N\overline{\*{H}}'$, we obtain
\begin{equation}\label{fhat0}
    \*{D}_N\overline{\*{H}}'\widehat{\*{f}}_t=\widehat{\*{f}}^0_t= \*{D}_N\overline{\*{H}}'\overline{\+{\Lambda}}'\*{f}_t+ \*{D}_N\overline{\*{H}}'\overline{\*e}_t=\*{f}^0_t+\overline{\*e}^0_t,
\end{equation}
where $\*{f}^0_t=[\*{f}'_t, \*{0}_{(m+1-r)\times 1}']'$ and $\overline{\*e}^0_t=[\overline{\*e}_{t}'\overline{\*H}_r, \sqrt{N}\overline{\*{e}}_{t}'\overline{\*H}_{-r}]'=
[\overline{\*{e}}^{0\prime}_{r,t}, \overline{\*{e}}^{0\prime}_{-r,t}]'$ are both $(m+1)\times 1$ with $\overline{\*{e}}_{r,t}^0$ and $\overline{\*{e}}_{-r,t}^0$ being $r\times 1$ and $(m+1-r)\times 1$, respectively. Hence, since $\|\overline{\*{e}}_{r,t}^0\| =O_p(N^{-1/2})$ and $\|\overline{\*{e}}_{-r,t}^0\| =O_p(1)$, when $m + 1> r$ we are no longer estimating $\*{f}_t$ but rather $\*f_t^+ = [\*{f}_t', \overline{\*{e}}_{-r,t}^{0\prime}]'$;
\begin{align}
\widehat{\*{f}}_t^0 = \*f_t^{0} + \overline{\*e}_t^{0} = \left[\begin{array}{c} \*f_t \\
    \*{0}_{(m+1-r)\times 1} \end{array}\right] + \left[\begin{array}{c} \overline{\*e}_{r,t}^{0} \\
    \overline{\*e}_{-r,t}^{0} \end{array}\right] = \*f_t^+ +  O_p(N^{-1/2}),
\end{align}
The fact that $\*{f}_t$ is included in $\*f_t^+$ suggests that asymptotically C$^2$ED$^2$ should be able to account for the unknown factors even if $m + 1> r$. By ensuring the existence of $\overline{\*{H}}$, Assumption 6 makes this possible. However, we also note that because of the presence of $\overline{\*{e}}_{-r,t}^0$, the asymptotic distribution theory will in general depend on whether $m+1=r$ or $m +1> r$.

It is useful to be able to use the above notation not only when $m+1> r$ but also when $m+1=r$. We therefore define $\widehat{\*{f}}^0_t=\overline{\+{\Lambda}}^{-1\prime}\widehat{\*{f}}_t$, $\*{f}^0_t = \*{f}_t$ and $\overline{\*{e}}^0_t = \overline{\+{\Lambda}}^{-1\prime}\overline{\*{e}}_t$ if $m+1=r$, so that we are back in (\ref{redefined}).

Let us now consider $\widehat \Delta_{i,g,t}$, which, unlike $\widehat{\*f}_t$, is computed based on treated units in post-treatment periods ($i\in \mathcal{I}_g \subset \mathcal{G}$ and $t \geq g_{\min}$). Note first that because we are considering treated units in post-treatment periods, $y_{i,t}(g) = y_{i,t}$ and $\*x_{i,t}(g) = \*x_{i,t}$. Further use of the definitions of $\Delta_{i,g,t}$, $y_{i,t}(\infty)$ and $\+\tau_{i,g,t}$, leads to the following model for $y_{i,t}$:
\begin{align}
y_{i,t} &= \Delta_{i,g,t} + y_{i,t}(\infty)  = \Delta_{i,g,t} + \+\beta_i'\*x_{i,t}(\infty) +  \+\alpha_i'\*f_t + \varepsilon_{i,t}\notag\\
& = \Delta_{i,g,t} + \+\beta_i'(\*x_{i,t}-\+\tau_{i,g,t}) +  \+\alpha_i'\*f_t + \varepsilon_{i,t} = (\Delta_{i,g,t} - \+\beta_i'\+\tau_{i,g,t}) + \+\beta_i'\*x_{i,t} + \+\alpha_i'\*f_t + \varepsilon_{i,t} \notag\\
&  = \eta_{i,g,t} + \+\beta_i'\*x_{i,t} + \+\alpha_i'\*f_t + \varepsilon_{i,t}.
\end{align}
It follows that
\begin{align}
\widehat \Delta_{i,g,t} & = y_{i,t} - \widehat y_{i,t}(\infty) \notag\\
& = \eta_{i,g,t} + \+\beta_i'\*x_{i,t} + \+\alpha_i'\*f_t + \varepsilon_{i,t} - [\widehat{\+\beta}'\widehat{\*x}_{i,t}(\infty) + \widehat{\*a}_i'\widehat{\*f}_t]  \notag\\
& = \eta_{i,g,t} + \+\beta_i'\*x_{i,t} + \+\alpha_i'\*f_t + \varepsilon_{i,t} - (\widehat{\+\beta}'\*x_{i,t} + \widehat{\*a}_i'\widehat{\*f}_t) + \widehat{\+\beta}'[\*x_{i,t} - \widehat{\*x}_{i,t}(\infty)]  \notag\\
& = \eta_{i,g,t} - (\widehat{\+\beta} - \+\beta_i)'\*x_{i,t} - (\widehat{\*a}_i'\widehat{\*f}_t - \+\alpha_i'\*f_t)  + \widehat{\+\beta}'[\*x_{i,t} - \widehat{\*x}_{i,t}(\infty)] + \varepsilon_{i,t} .
\end{align}
Consider $\widehat{\*a}_i'\widehat{\*f}_t - \+\alpha_i'\*f_t$. While the $(m+1)\times r$ matrix $\mathbf{D}_N\overline{\mathbf{H}}'\overline{\boldsymbol{\Lambda}}'$ is not necessarily square under Assumption 6, it has full column rank. This means that we can compute its Moore--Penrose inverse, which is given by $(\mathbf{D}_N\overline{\mathbf{H}}'\overline{\boldsymbol{\Lambda}}')^+ = (\mathbf{D}_N\overline{\mathbf{H}}'\overline{\boldsymbol{\Lambda}}')' = [ \mathbf{I}_r, \mathbf{0}_{r\times (m+1-r)} ]$, such that $(\mathbf{D}_N\overline{\mathbf{H}}'\overline{\boldsymbol{\Lambda}}')^+ \mathbf{D}_N\overline{\mathbf{H}}'\overline{\boldsymbol{\Lambda}}' = \mathbf{I}_r$. Hence, $\mathbf{D}_N\overline{\mathbf{H}}'\overline{\boldsymbol{\Lambda}}'\*f_t = [\*f_t', \mathbf{0}_{(m+1-r)\times 1}']' = \*f_t^0$ and we also have $\*{D}_N\overline{\*{H}}'\widehat{\*{f}}_t=\widehat{\*{f}}^0_t$. Making use of this, and letting $\widehat{\*a}_i^0 = (\*{D}_N\overline{\*{H}}')^{-1\prime}\widehat{\*a}_i= (\overline{\*{H}}\*{D}_N)^{-1}\widehat{\*a}_i$ and $\+\alpha_i^0 = (\mathbf{D}_N\overline{\mathbf{H}}'\overline{\boldsymbol{\Lambda}}')^{+\prime} \+\alpha_i= \mathbf{D}_N\overline{\mathbf{H}}'\overline{\boldsymbol{\Lambda}}'\+\alpha_i = [ \+\alpha_i ', \mathbf{0}_{1\times (m+1-r)} ]'$,
\begin{align}
\widehat{\*a}_i'\widehat{\*f}_t - \+\alpha_i'\*f_t &= \widehat{\*a}_i'(\*{D}_N\overline{\*{H}}')^{-1}\*{D}_N\overline{\*{H}}'\widehat{\*f}_t - \+\alpha_i'(\mathbf{D}_N\overline{\mathbf{H}}'\overline{\boldsymbol{\Lambda}}')^+\mathbf{D}_N\overline{\mathbf{H}}'\overline{\boldsymbol{\Lambda}}'\*f_t \notag\\
& = \widehat{\*a}_i^{0\prime}\widehat{\*f}_t^0 - \+\alpha_i^{0\prime}\*f_t^0 \notag\\
& = \+\alpha_i^{0\prime}(\widehat{\*f}_t^0-\*f_t^0) + (\widehat{\*a}_i^{0} - \+\alpha_i^0)'\widehat{\*f}_t^0,
\end{align}
from which it follows that
\begin{align}
\widehat \Delta_{i,g,t}  = \eta_{i,g,t} - (\widehat{\+\beta} - \+\beta_i)'\*x_{i,t}  - \+\alpha_i^{0\prime}(\widehat{\*f}_t^0-\*f_t^0) - (\widehat{\*a}_i^{0} - \+\alpha_i^0)'\widehat{\*f}_t^0  + \widehat{\+\beta}'[\*x_{i,t} - \widehat{\*x}_{i,t}(\infty)] + \varepsilon_{i,t}.
\end{align}
Amongst the terms appearing on the right-hand side of this last equation, the one involving $\widehat{\*a}_i^0 - \+\alpha_i^0$ requires most work. We therefore start with this. Note first that since $\widehat{\*a}_i$ is estimated based on the pre-treatment period only, we have $y_{i,t} = y_{i,t}(\infty) = \+\beta_i'\*x_{i,t}(\infty) +  \+\alpha_i'\*f_t + \varepsilon_{i,t}$ or, in terms of the stacked vector notation introduced in step 2 of the counterfactual estimation procedure outlined in the main paper, $\*y_{i}= \*x_{i}\+\beta_i + \*f\+\alpha_i+  \+\varepsilon_{i}$, where $\*y_{i}$, $\*x_{i}$, $\*f$ and $\+\varepsilon_{i}$ are all $(g_{\min}-1)$-rowed. By using this and $\overline{\+\Lambda}\overline{\*H}_{r} = \*I_{r}$, we get
\begin{eqnarray}
\*y_{i}= \*x_{i}\+\beta_i + \widehat{\*f}\overline{\*H}_{r}\+\alpha_i - (\widehat{\*f} - \*f\overline{\+\Lambda})\overline{\*H}_{r}\+\alpha_i+  \+\varepsilon_{i}  = \*x_{i}\+\beta_i + \widehat{\*f} \overline{\*H}_{r}\+\alpha_i - \overline{\*e}_{r}^0\+\alpha_i +  \+\varepsilon_{i}. \label{ystack}
\end{eqnarray}
We also note that $\*a_i$ in step 2 can be expressed in terms of $\overline{\*H}_{r}$ and $\+\alpha_i$ as $\*a_i = \overline{\*H}_{r}\+\alpha_i$. By inserting this and \eqref{ystack} into the expression given for $\widehat{\*a}_i$ in step 2,
\begin{align}
\widehat{\*a}_i &= (\widehat{\*f}'\widehat{\*f})^{-1}\widehat{\*f}'(\*y_{i}-\*x_{i}\widehat{\+\beta}) \notag\\
& = (\widehat{\*f}'\widehat{\*f})^{-1}\widehat{\*f}'(\*x_{i}\+\beta_i + \widehat{\*f} \*a_i - \overline{\*e}_{r}^0\+\alpha_i +  \+\varepsilon_{i}-\*x_{i}\widehat{\+\beta}) \notag\\
& =  \*a_i + (\widehat{\*f}'\widehat{\*f})^{-1}\widehat{\*f}'[- \*x_{i}(\widehat{\+\beta}-\+\beta_i)  - \overline{\*e}_{r}^0\+\alpha_i +  \+\varepsilon_{i} ],
\end{align}
implying
\begin{align}
\widehat{\*a}_i^0 & = (\overline{\*{H}}\*{D}_N)^{-1}\widehat{\*a}_i \notag\\
& = (\overline{\*{H}}\*{D}_N)^{-1}\*a_i + (\overline{\*{H}}\*{D}_N)^{-1}(\widehat{\*f}'\widehat{\*f})^{-1}\widehat{\*f}'[- \*x_{i}(\widehat{\+\beta}-\+\beta_i) - \overline{\*e}_{r}^0\+\alpha_i +  \+\varepsilon_{i} ] \notag\\
& = (\overline{\*{H}}\*{D}_N)^{-1}\*a_i +  (
\*{D}_N\overline{\*{H}}'\widehat{\*f}'\widehat{\*f}\overline{\*{H}}\*{D}_N)^{-1}\*{D}_N\overline{\*{H}}'\widehat{\*f}' [ - \*x_{i}(\widehat{\+\beta}-\+\beta_i)  - \overline{\*e}_{r}^0\+\alpha_i +  \+\varepsilon_{i}]
\notag\\
& = (\overline{\*{H}}\*{D}_N)^{-1}\*a_i + (\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}^{0\prime}[- \*x_{i}(\widehat{\+\beta}-\+\beta_i)  - \overline{\*e}_{r}^0\+\alpha_i +  \+\varepsilon_{i} ]
\end{align}
where $\widehat{\*f}^0 = [\widehat{\*f}_1^0,...,\widehat{\*f}_{g_{\min}-1}^0]' = \widehat{\*f}\overline{\*{H}}\*{D}_N$ is $(g_{\min}-1)\times (m+1)$. Consider the first term on the right-hand side. A direct calculation using the rules for the inverse of a partitioned matrix (see, for example, \citet{abadir2005matrix}, Exercise 5.16) reveals that
\begin{equation}
    (\*{D}_N\overline{\*{H}})^{-1} =\left[\begin{array}{cc}\overline{\+{\Lambda}}_r & \overline{\+{\Lambda}}_{-r}\\
    \*{0}_{(m+1-r)\times r} & N^{-1/2}\*{I}_{m+1-r}\end{array}\right],
\end{equation}
so that
\begin{align}
(\overline{\*{H}}\*{D}_N)^{-1}\overline{\*H}_{r} = \left[\begin{array}{cc}\overline{\+{\Lambda}}_r & \overline{\+{\Lambda}}_{-r}\\
    \*{0}_{(m+1-r)\times r} & N^{-1/2}\*{I}_{m+1-r}\end{array}\right]\left[\begin{array}{c}\overline{\+{\Lambda}}_r^{-1} \\
    \*{0}_{(m+1-r)\times r} \end{array}\right] = \left[\begin{array}{c} \*I_r \\
    \*{0}_{(m+1-r)\times r} \end{array}\right].
\end{align}
This implies
\begin{align}
(\overline{\*{H}}\*{D}_N)^{-1}\*a_i = \left[\begin{array}{c} \+\alpha_i \\
    \*{0}_{(m+1-r)\times 1} \end{array}\right] = \+\alpha_i^0,
\end{align}
leading to the following expression for $\widehat{\*a}_i^0-\+\alpha_i^0$:
\begin{align}
\widehat{\*a}_i^0 - \+\alpha_i^0 & = (\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}^{0\prime}[- \*x_{i}(\widehat{\+\beta}-\+\beta_i)  - \overline{\*e}_{r}^0\+\alpha_i +  \+\varepsilon_{i} ].
\end{align}
We similarly have
\begin{equation}
\widehat{\*x}_{i,t}(\infty) = \widehat{\+\lambda}_i'\widehat{\*f}_t = \*x_i'\widehat{\*f}( \widehat{\*f}' \widehat{\*f} )^{-1} \widehat{\*f}_t= \*x_i'\widehat{\*f}^0( \widehat{\*f}^{0\prime} \widehat{\*f}^0 )^{-1} \widehat{\*f}_t^0 ,
\end{equation}
from which it follows that
\begin{equation}
\widehat{\+\beta}'[\*x_{i,t} - \widehat{\*x}_{i,t}(\infty)] = \widehat{\+\beta}'[\*x_{i,t} - \*x_i'\widehat{\*f}^0( \widehat{\*f}^{0\prime} \widehat{\*f}^0 )^{-1} \widehat{\*f}_t^0].
\end{equation}
By inserting the above expressions into the one given earlier for $\widehat \Delta_{i,g,t}$, we get
\begin{align}
\widehat \Delta_{i,g,t} & = \eta_{i,g,t} - (\widehat{\+\beta} - \+\beta_i)'\*x_{i,t} - \+\alpha_i^{0\prime}(\widehat{\*f}_t^0-\*f_t^0) - (\widehat{\*a}_i^{0} - \+\alpha_i^0)'\widehat{\*f}_t^0 + \widehat{\+\beta}'[\*x_{i,t} - \widehat{\*x}_{i,t}(\infty)] + \varepsilon_{i,t}\notag\\
& = \eta_{i,g,t} - (\widehat{\+\beta} - \+\beta_i)'\*x_{i,t} - \+\alpha_i'\overline{\*e}_{r,t}^0 - [- \*x_{i}(\widehat{\+\beta}-\+\beta_i)  - \overline{\*e}_{r}^0\+\alpha_i +  \+\varepsilon_{i} ]' \widehat{\*f}^{0} (\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}_t^0\notag\\
& + \widehat{\+\beta}'[\*x_{i,t} - \*x_i'\widehat{\*f}^0( \widehat{\*f}^{0\prime} \widehat{\*f}^0 )^{-1} \widehat{\*f}_t^0] + \varepsilon_{i,t}\notag\\
& = \eta_{i,g,t} + \+\beta_i'\*x_{i,t} - \+\alpha_i'\overline{\*e}_{r,t}^0 + \varepsilon_{i,t} - ( \*x_{i}\+\beta_i  - \overline{\*e}_{r}^0\+\alpha_i +  \+\varepsilon_{i} )' \widehat{\*f}^{0} (\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}_t^0  .
\end{align}
Further use of $\widehat{\*f} = \widehat{\*f}^0\*D_{N}^{-1}\overline{\*H}^{-1}$ gives
\begin{eqnarray}
\*x_i = \*f\+\lambda_i + \*v_i = \widehat{\*f}\overline{\*H}_{r}\+\lambda_i -  (\widehat{\*f} - \*f\overline{\+\Lambda})\overline{\*H}_{r}\+\lambda_i  + \*v_i =  \widehat{\*f}^0\*D_{N}^{-1}\overline{\*H}^{-1}\overline{\*H}_{r}\+\lambda_i  - \overline{\*e}_{r}^0\+\lambda_i  + \*v_i ,
\end{eqnarray}
for $t< g_{\min}$. If, on the other hand, $t \geq g_{\min}$, then
\begin{eqnarray}
\*x_{i,t} = \+\tau_{i,g,t} + \+\lambda_i'\*f_t + \*v_{i,t} =  \+\tau_{i,g,t} + \+\lambda_i' \overline{\*H}_{r}'\overline{\*H}^{-1\prime}\*D_{N}^{-1}\widehat{\*f}_t^0  - \+\lambda_i'\overline{\*e}_{r,t}^0  + \*v_{i,t}.
\end{eqnarray}
These two last results imply
\begin{align}
& \*x_{i,t} -  \*x_{i}' \widehat{\*f}^{0}(\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}_t^0 \notag\\
& = \+\tau_{i,g,t} + \+\lambda_i' \overline{\*H}_{r}'\overline{\*H}^{-1\prime}\*D_{N}^{-1}\widehat{\*f}_t^0  - \+\lambda_i'\overline{\*e}_{r,t}^0  + \*v_{i,t} -  (\widehat{\*f}^0\*D_{N}^{-1}\overline{\*H}^{-1}\overline{\*H}_{r}\+\lambda_i  - \overline{\*e}_{r}^0\+\lambda_i  + \*v_i)' \widehat{\*f}^{0}(\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}_t^0\notag\\
& = \+\tau_{i,g,t}  - \+\lambda_i'\overline{\*e}_{r,t}^0  + \*v_{i,t} -  ( - \overline{\*e}_{r}^0\+\lambda_i  + \*v_i)' \widehat{\*f}^{0}(\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}_t^0,
\end{align}
and so we arrive at the following expression for $\widehat \Delta_{i,g,t}$:
\begin{align}
\widehat \Delta_{i,g,t} & = \eta_{i,g,t} + \+\beta_i'(\+\tau_{i,g,t}  - \+\lambda_i'\overline{\*e}_{r,t}^0  + \*v_{i,t}) - \+\alpha_i'\overline{\*e}_{r,t}^0 + \varepsilon_{i,t}\notag\\
& - [ (- \overline{\*e}_{r}^0\+\lambda_i  + \*v_i)\+\beta_i  - \overline{\*e}_{r}^0\+\alpha_i +  \+\varepsilon_{i} ]' \widehat{\*f}^{0} (\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}_t^0 \notag\\
& = \Delta_{i,g,t} - (\+\lambda_i\+\beta_i + \+\alpha_i)'\overline{\*e}_{r,t}^0 + \+\beta_i'\*v_{i,t} + \varepsilon_{i,t} \notag\\
& - [ - \overline{\*e}_{r}^0(\+\lambda_i\+\beta_i +\+\alpha_i) + \*v_i\+\beta_i+  \+\varepsilon_{i} ]' \widehat{\*f}^{0} (\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}_t^0  .
\end{align}
where $\Delta_{i,g,t} = \eta_{i,g,t} + \+\beta_i'\+\tau_{i,g,t}$ as in the main paper.

The above expression for $\widehat \Delta_{i,g,t}$ is the cleanest possible without exploiting the fact that $N$ is large. Hence, in what remains we are going to let $N\to\infty$. We begin by considering $\widehat{\*f}^{0} (\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}_t^0$. Define $\*f^+ = [\*f_1^+,... \*f_{g_{\min}-1}^+]' = [\*f, \overline{\*e}_{-r}^{0}]$, a $(g_{\min}-1)\times (m+1)$ matrix. We have already shown that $\widehat{\*{f}}^0 = \*f^+ +  O_p(N^{-1/2})$. By using this and the results provided in the proof of Lemma A.1 in \citet{westerlund2019cce}, we have that $\|\widehat{\*f}^{0\prime}\widehat{\*f}^0 - \*f^{+\prime}\*f^+\|= O_p(N^{-1/2})$ and, more importantly,
\begin{align}
\|(\widehat{\*f}^{0\prime}\widehat{\*f}^0 )^{-1} - (\*f^{+\prime}\*f^+)^{-1}\| = O_p(N^{-1/2}),
\end{align}
where
\begin{align}
\*f^{+\prime}\*f^+ &= \left[\begin{array}{cc} \*f'\*f & \*f'\overline{\*e}_{-r}^0 \\ \overline{\*e}_{-r}^{0\prime}\*f  & \overline{\*e}_{-r}^{0\prime}\overline{\*e}_{-r}^0 \end{array}\right] ,\\
(\*f^{+\prime}\*f^+)^{-1} &= \left[\begin{array}{c} (\*f'\*f)^{-1} + (\*f'\*f)^{-1}\*f'\overline{\*e}_{-r}^0(\overline{\*e}_{-r}^{0\prime}\*M_{\*f}\overline{\*e}_{-r}^0)^{-1}\overline{\*e}_{-r}^{0\prime} \*f(\*f'\*f)^{-1}  \\
-(\overline{\*e}_{-r}^{0\prime}\*M_{\*f}\overline{\*e}_{-r}^0)^{-1}\overline{\*e}_{-r}^{0\prime}\*f (\*f'\*f)^{-1} \end{array}\right. \notag\\
& \left. \begin{array}{c}  -(\*f'\*f)^{-1}\*f'\overline{\*e}_{-r}^0(\overline{\*e}_{-r}^{0\prime}\*M_{\*f}\overline{\*e}_{-r}^0)^{-1} \\
 (\overline{\*e}_{-r}^{0\prime}\*M_{\*f}\overline{\*e}_{-r}^0)^{-1} \end{array}\right].
\end{align}
The expression for $(\*f^{+\prime}\*f^+)^{-1}$ is again obtained by using the rules for the inverse of a partitioned matrix. The fact that $\|(\widehat{\*f}^{0\prime}\widehat{\*f}^0 )^{-1} - (\*f^{+\prime}\*f^+)^{-1}\| = O_p(N^{-1/2})$ together with $\widehat{\*{f}}^0 = \*f^+ +  O_p(N^{-1/2})$ imply that
\begin{align}
\widehat{\*f}_t^{0\prime}(\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}^{0\prime} & =  \widehat{\*f}_t^{0\prime}[(\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1} - (\*f^{+\prime}\*f^+)^{-1} ]\widehat{\*f}^{0\prime} + \widehat{\*f}_t^{0\prime}(\*f^{+\prime}\*f^+)^{-1}\widehat{\*f}^{0\prime} \notag\\
& = \widehat{\*f}_t^{0\prime}(\*f^{+\prime}\*f^+)^{-1}\widehat{\*f}^{0\prime} + O_p(N^{-1/2})\notag\\
& = \*f_t^{+\prime}(\*f^{+\prime}\*f^+)^{-1}\*f^{+\prime} + O_p(N^{-1/2}).
\end{align}
where, defining $\*M_{\*f}$ analogously to $\*M_{\widehat{\*f}}$,
\begin{align}
& \*f_t^{+\prime}(\*f^{+\prime}\*f^+)^{-1}\*f^{+\prime} \notag\\
& = [\*f_t', \overline{\*e}_{-r,t}^{0\prime}]\left[\begin{array}{c} (\*f'\*f)^{-1} + (\*f'\*f)^{-1}\*f'\overline{\*e}_{-r}^0(\overline{\*e}_{-r}^{0\prime}\*M_{\*f}\overline{\*e}_{-r}^0)^{-1}\overline{\*e}_{-r}^{0\prime} \*f(\*f'\*f)^{-1}  \\
-(\overline{\*e}_{-r}^{0\prime}\*M_{\*f}\overline{\*e}_{-r}^0)^{-1}\overline{\*e}_{-r}^{0\prime}\*f (\*f'\*f)^{-1} \end{array}\right. \notag\\
& \left. \begin{array}{c}  -(\*f'\*f)^{-1}\*f'\overline{\*e}_{-r}^0(\overline{\*e}_{-r}^{0\prime}\*M_{\*f}\overline{\*e}_{-r}^0)^{-1} \\
 (\overline{\*e}_{-r}^{0\prime}\*M_{\*f}\overline{\*e}_{-r}^0)^{-1} \end{array}\right]\left[\begin{array}{c} \*f' \\
    \overline{\*e}_{-r}^{0\prime} \end{array}\right]  \notag\\
& = \*f_t'(\*f'\*f)^{-1}\*f'[\*I_{g_{\min}-1} -  \overline{\*e}_{-r}^{0}(\overline{\*e}_{-r}^{0\prime}\*M_{\*f}\overline{\*e}_{-r}^0)^{-1} \overline{\*e}_{-r}^{0\prime}\*M_{\*f}] + \overline{\*e}_{-r,t}^{0\prime}(\overline{\*e}_{-r}^{0\prime}\*M_{\*f}\overline{\*e}_{-r}^0)^{-1}\overline{\*e}_{-r}^{0\prime}\*M_{\*f}.
\end{align}
The fact that $\|\widehat{\*f}_t^{0\prime}(\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}^{0\prime} - \*f_t^{+\prime}(\*f^{+\prime}\*f^+)^{-1}\*f^{+\prime}\| = O_p(N^{-1/2})$ implies
\begin{align}
\widehat \Delta_{i,g,t} & = \Delta_{i,g,t} - (\+\lambda_i\+\beta_i + \+\alpha_i)'\overline{\*e}_{r,t}^0 + \+\beta_i'\*v_{i,t} + \varepsilon_{i,t} \notag\\
& - [ - \overline{\*e}_{r}^0(\+\lambda_i\+\beta_i +\+\alpha_i) + \*v_i\+\beta_i+  \+\varepsilon_{i} ]'  \*f^{+} (\*f^{+\prime}\*f^+)^{-1}\*f_t^+ + O_p(N^{-1/2})\notag\\
& = \Delta_{i,g,t} - (\+\lambda_i\+\beta_i + \+\alpha_i)'\overline{\*e}_{r,t}^{0\ast} + \+\beta_i'\*v_{i,t}^* + \varepsilon_{i,t}^*  + O_p(N^{-1/2}),
\end{align}
where
\begin{align}
\*a_{i,t}^* = \*a_{i,t} - \*a_i' \*f^{+} (\*f^{+\prime}\*f^+)^{-1}\*f_t^+ = \*a_{i,t} - \sum_{s=1}^{g_{\min}-1} \*a_{i,s}\*f_s^{+\prime} (\*f^{+\prime}\*f^+)^{-1}\*f_t^+
\end{align}
for any vector $\*a_{i,t}$ with $(g_{\min}-1)$-rowed stack $\*a_i = [\*a_{i,1},...,\*a_{i,g_{\min}-1}]'$. In words, $\*a_{i,t}^*$ is the limiting ``defactored'' version of $\*a_{i,t}$.

We now make use of the above expression for $\widehat \Delta_{i,g,t}$ to evaluate $\widehat \Delta_{g,t}$. In so doing, it is important to note that the order of the reminder incurred when replacing $\widehat{\*f}_t^{0\prime}(\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}^{0\prime}$ with $\*f_t^{+\prime}(\*f^{+\prime}\*f^+)^{-1}\*f^{+\prime}$ is the same even after averaging over group $g$ and multiplying by $\sqrt{|\mathcal{I}_g|}$. In order to appreciate this, we make use of the fact that $\|\sqrt{|\mathcal{I}_g|}\overline{\*e}_{r}^0\| = O_p(1)$, and since $\*v_{i}$ and $\+\beta_i$ are independent with $\*v_i$ mean zero and independent also across $i$, we also have $\|(|\mathcal{I}_g|)^{-1/2}\sum_{i\in \mathcal{I}_g} \*v_{i}\+\beta_i\| = O_p(1)$. It follows that
\begin{align}
& \left\|\frac{1}{\sqrt{|\mathcal{I}_g|}}\sum_{i\in \mathcal{I}_g}[ - \overline{\*e}_{r}^0(\+\lambda_i\+\beta_i +\+\alpha_i) + \*v_i\+\beta_i+  \+\varepsilon_{i} ] \right\|\notag\\
& \leq \|\sqrt{|\mathcal{I}_g|}\overline{\*e}_{r}^0\|\left\|\frac{1}{|\mathcal{I}_g|} \sum_{i \in \mathcal{I}_g} (\+\lambda_i\+\beta_i +\+\alpha_i)\right\| + \left\| \frac{1}{\sqrt{|\mathcal{I}_g|}} \sum_{i\in \mathcal{I}_g} \*v_{i}\+\beta_i \right\| +  \left\|\frac{1}{\sqrt{|\mathcal{I}_g|}} \sum_{i \in \mathcal{I}_g} \+\varepsilon_{i}\right\| =O_p(1).
\end{align}
We can therefore show that
\begin{align}
& \left\|\frac{1}{\sqrt{|\mathcal{I}_g|}}\sum_{i \in \mathcal{I}_g} [ - \overline{\*e}_{r}^0(\+\lambda_i\+\beta_i +\+\alpha_i) + \*v_i\+\beta_i+  \+\varepsilon_{i} ]'[\*f^{+} (\*f^{+\prime}\*f^+)^{-1}\*f_t^+ - \widehat{\*f}^{0} (\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}_t^0] \right\| \notag\\
& \leq
\left\| \frac{1}{\sqrt{|\mathcal{I}_g|}}\sum_{i \in \mathcal{I}_g}[ - \overline{\*e}_{r}^0(\+\lambda_i\+\beta_i +\+\alpha_i) + \*v_i\+\beta_i+  \+\varepsilon_{i} ]\right\| \|\*f^{+} (\*f^{+\prime}\*f^+)^{-1}\*f_t^+ - \widehat{\*f}^{0} (\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}_t^0\|\notag\\
& = O_p(N^{-1/2}),
\end{align}
which means that the reminder incurred when replacing $\widehat{\*f}_t^{0\prime}(\widehat{\*f}^{0\prime}\widehat{\*f}^0)^{-1}\widehat{\*f}^{0\prime}$ with $\*f_t^{+\prime}(\*f^{+\prime}\*f^+)^{-1}\*f^{+\prime}$ is $O_p(N^{-1/2})$ after averaging over group $g$ and multiplying by $\sqrt{|\mathcal{I}_g|}$.

For $\Delta_{i,g,t}$, we make use of the fact that $\Delta_{i,g,t}= \Delta_{g,t} + \upsilon_{i,t}$ for $i\in \mathcal{I}_g$ by Assumption 3, giving
\begin{align}
 \sqrt{|\mathcal{I}_g|}(\widehat \Delta_{g,t} - \Delta_{g,t}) & = \frac{1}{\sqrt{|\mathcal{I}_g|}} \sum_{i\in \mathcal{I}_g} (\widehat \Delta_{i,g,t} - \Delta_{g,t}) \notag\\
& = \frac{1}{\sqrt{|\mathcal{I}_g|}} \sum_{i\in \mathcal{I}_g} (\widehat \Delta_{i,g,t} - \Delta_{i,g,t} + \upsilon_{i,t}) \notag\\
& = \frac{1}{\sqrt{|\mathcal{I}_g|}} \sum_{i\in \mathcal{I}_g} [\upsilon_{i,t} - (\+\lambda_i\+\beta_i + \+\alpha_i)'\overline{\*e}_{r,t}^{0\ast} + \+\beta_i'\*v_{i,t}^* + \varepsilon_{i,t}^* ] + O_p(N^{-1/2}) .
\end{align}
Moreover, $|\mathcal{I}_g|/N \to_p \tau_g \in (0,1)$ by Assumption 2. Hence, if we in addition define $\*a_g = \mathrm{plim}_{N\to\infty} (|\mathcal{I}_g|)^{-1} \sum_{i\in \mathcal{I}_g}(\+\lambda_i\+\beta_i + \+\alpha_i)$, the above expression for $\sqrt{|\mathcal{I}_g|}(\widehat \Delta_{g,t} - \Delta_{g,t})$ becomes
\begin{align}
& \sqrt{|\mathcal{I}_g|}(\widehat \Delta_{g,t} - \Delta_{g,t}) \notag\\
&= \frac{1}{\sqrt{|\mathcal{I}_g|}}\sum_{i\in \mathcal{I}_g}(\upsilon_{i,t}  + \+\beta_i'\*v_{i,t}^* + \varepsilon_{i,t}^*) - \sqrt{\frac{|\mathcal{I}_g|}{N}} \frac{1}{|\mathcal{I}_g|}\sum_{i\in \mathcal{I}_g} (\+\lambda_i\+\beta_i + \+\alpha_i)'\sqrt{N}\overline{\*e}_{r,t}^{0\ast} + O_p(N^{-1/2}) \notag\\
&= \frac{1}{\sqrt{|\mathcal{I}_g|}}\sum_{i\in \mathcal{I}_g} (\upsilon_{i,t}  + \+\beta_i'\*v_{i,t}^* + \varepsilon_{i,t}^* ) - \sqrt{\tau_g}\*a_g'\sqrt{N}\overline{\*e}_{r,t}^{0*} + o_p(1).
\end{align}
All the terms on the right-hand side of the above equation are mean zero and independent across $i$ (conditionally on $\*f$). They are therefore asymptotically normal by a central limit law for independent variables. However, they are not uncorrelated with each other, which complicates the calculation of the asymptotic variance. Let us therefore define $\sigma^2(\widehat \Delta_{g,t}) = \mathrm{var}(\sqrt{|\mathcal{I}_g|}(\widehat \Delta_{g,t} - \Delta_{g,t})  |\mathcal{C})$, where $\mathcal{C}$ is the sigma-field generated by $\*f$. The asymptotic distribution of $\sqrt{|\mathcal{I}_g|}(\widehat \Delta_{g,t} - \Delta_{g,t})$ as $N\to\infty$ can now be stated in the following way:
\begin{align}
\sqrt{|\mathcal{I}_g|}(\widehat \Delta_{g,t} - \Delta_{g,t}) \to_d MN(0, \sigma^2(\widehat \Delta_{g,t}) ),
\end{align}
where $MN(\cdot,\cdot)$ signifies a mixed normal distribution that is normal conditionally on $\mathcal{C}$. This means that the conditional distribution of $\sqrt{|\mathcal{I}_g|}(\widehat \Delta_{g,t} - \Delta_{g,t})/\sigma(\widehat \Delta_{g,t})$ is also the unconditional distribution. Hence,
\begin{align}
\frac{\sqrt{|\mathcal{I}_g|}(\widehat \Delta_{g,t} - \Delta_{g,t})}{\sigma(\widehat \Delta_{g,t})} \to_d N(0, 1),
\end{align}
as required for part (a).

It remains to prove (b) and the consistency of $\widehat \sigma^2(\widehat \Delta_{g,t})$. From before,
\begin{align}
\widehat \Delta_{i,g,t} & = \Delta_{g,t} + \upsilon_{i,t} - (\+\lambda_i\+\beta_i + \+\alpha_i)'\overline{\*e}_{r,t}^{0\ast} + \+\beta_i'\*v_{i,t}^* + \varepsilon_{i,t}^* + O_p(N^{-1/2}), \\
\frac{1}{|\mathcal{I}_g|}\sum_{i\in \mathcal{I}_g} \widehat \Delta_{i,g,t} & = \Delta_{g,t}+ \frac{1}{|\mathcal{I}_g|}\sum_{i\in \mathcal{I}_g} [\upsilon_{i,t} - (\+\lambda_i\+\beta_i + \+\alpha_i)'\overline{\*e}_{r,t}^{0\ast} + \+\beta_i'\*v_{i,t}^* + \varepsilon_{i,t}^*] + O_p(N^{-1/2}).
\end{align}
It follows that if we let $z_{i,t} = \upsilon_{i,t} - (\+\lambda_i\+\beta_i + \+\alpha_i)'\overline{\*e}_{r,t}^{0\ast} + \+\beta_i'\*v_{i,t}^* + \varepsilon_{i,t}^*$, then
\begin{align}
\widehat \Delta_{i,g,t}  - \frac{1}{|\mathcal{I}_g|}\sum_{j \in \mathcal{I}_g} \widehat \Delta_{j,g,t} & = z_{i,t}  - \frac{1}{|\mathcal{I}_g|}\sum_{j\in \mathcal{I}_g} z_{j,t} + O_p(N^{-1/2}).
\end{align}
Hence, since $z_{i,t}$ is again independent across $i$, by a law of large numbers for independent variables,
\begin{align}
\widehat \sigma^2(\widehat \Delta_{g,t}) & = \frac{1}{|\mathcal{I}_g|-1}\sum_{i\in \mathcal{I}_g}\left(\widehat \Delta_{i,g,t}  - \frac{1}{|\mathcal{I}_g|}\sum_{j\in \mathcal{I}_g} \widehat \Delta_{j,g,t}\right)^2 \notag\\
& = \frac{1}{|\mathcal{I}_g|-1} \sum_{i\in \mathcal{I}_g} \left(z_{i,t}  - \frac{1}{|\mathcal{I}_g|} \sum_{j \in \mathcal{I}_g} z_{j,t}\right)^2 + O_p(N^{-1/2}) \to_p \sigma^2(\widehat \Delta_{g,t})
\end{align}
as $N\to\infty$ (see \citealp{pesaran2006estimation}, page 985, for a similar argument). This establishes part (b) and hence the proof of the theorem is complete.

\section{Discussion of some of the assumptions}
\renewcommand{\theequation}{B.\arabic{equation}}

The results reported in the main paper assume that the covariates admits to a common factor representation, which is not needed in pricipal components-based studies such as that of \citet{chan2022pcdid}. In this section, we show that the direct effect is estimable even if this assumption fails, although in that case we can no longer identify the overall and indirect ATTs.

Our starting point here is \citet{Brown_Schmidt_Wooldridge_2021}, who consider the same CCE approach as in \citet{pesaran2006estimation} but under a different set of assumptions. In particular, instead of requiring that $\*x_{i,t}$ has factor structure, they assume that $\*f_t$ satisfies
\begin{equation}\label{bsw}
    \*f_t = \*B'\+\Psi_t
\end{equation}
where $\+\Psi_t = \mathbb{E}(\*z_{i,t})$ is constant in $i$ and $\*B$ is an arbitrary $(m+1)\times r$ matrix of constants. Unlike the factors-in-covariates condition, \eqref{bsw} is not testable. However, if it holds, $\widehat{\*f}_t$ can be used to estimate an arbitrary number of factors. In order to illustrate this last point, note that if \eqref{bsw} holds for the untreated potential outcomes,
\begin{align}
    y_{i,t}(\infty) &= \+\beta_i'\*x_{i,t}(\infty) +  \+\alpha_i'\*f_t + \epsilon_{i,t} \notag\\
    & = \+\beta_i'\*x_{i,t}(\infty) + \*a_i'\mathbb{E}(\*z_{i,t} | g_i = \infty)  + \epsilon_{i,t}\notag\\
    & = \+\beta_i'\*x_{i,t}(\infty) + \*a_i'\widehat{\*f}_t + \*a_i'[\mathbb{E}(\*z_{i,t} | g_i = \infty)- \widehat{\*f}_t]  + \epsilon_{i,t}
\end{align}
where $\*a_i = \*B\+\alpha_i$ and $\mathbb{E}(\*z_{i,t} | g_i = \infty)- \widehat{\*f}_t$ is negligible, as $\overline{\*z}_t \to_p \mathbb{E}(\*z_{i,t})$ as $N\to \infty$ under standard regulatory conditions.

The main advantage of \eqref{bsw} is that it leaves the covariates essentially unrestricted. However, because we no longer have a model for the untreated potential covariates, we cannot estimate $\*x_{i,t}(\infty)$ in step 3 of the counterfactual estimation procedure. This has two implications; (i) we are unable to identify the effect of the treatment on $\*x_{i,t}$, and (ii) we have to use $\*x_{i,t}$ as opposed to $\widehat{\*x}_{i,t}(\infty)$ when computing $\widehat{y}_{i,t}(\infty)$ in step 4. As a result, similarly to \citet{chan2022pcdid}, we can only identify the direct ATT. Hence, while we can relax factors-in-covariates condition, this has a price in terms of the estimable ATTs.

\pagebreak

\bibliography{references}

\end{document}




